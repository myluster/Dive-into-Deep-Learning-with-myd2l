# softmax回归
## 分类问题
- 回归可以用于**预测**多少的问题，也可以用于**分类**的问题
- 分类问题关心的不是"多少"而是"哪一个"，可以分为两个问题：
  - 【1】我们只对样本的硬分类感兴趣，即属于哪个类别
  - 【2】我们希望得到软分类，即得到属于每个类别的概率
- 两者界限非常模糊，因为即使我们只关心硬类别，我们仍然使用软类别的模型
## 独热编码
- 是一个向量，它的分量和类别一样多。类别对应的分量设置为1，其他所有分类设置为0
## 网络架构
- 为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。
- 对于线性模型的分类问题，我们需要输出一样多的仿射函数（affine function）。每个输出对应它自己的仿射函数，如下例：
![image](https://github.com/user-attachments/assets/42d8ee16-d586-418d-abb6-79220679e81a)<br>
统一为:
$o=WX+b$
- 对应的神经网络图如下：<br>
![image](https://github.com/user-attachments/assets/db6bede9-10ab-459b-948f-c09021242ef6)
- 与线性回归一样，softmax回归也是一个单层神经网络，由于计算每个输出取决于所有输入，所以softmax回归的输出层也是全连接层。

## 全连接层的参数开销：
- 	在深度学习中，全连接层是完全连接的，可能有很多可学习的参数，具体来说：
  - 对于输入维度为d、输出维度为q的全连接层，每个输入节点需与所有输出节点连接。
	-	参数总量为:d×q+b（含偏置项），忽略偏置项后，参数复杂度为O(dq)
	-	当d和q较大时（如高维特征或大规模分类任务），参数数量急剧增长，导致：内存占用过高、训练和推理速度下降、模型易过拟合（需更多数据支撑）<br>
		
	-	通过引入超参数n（如分组数、分解秩等），可将参数复杂度 降低至O(dq/n)。
	- 常见的实现方式为分组全连接（Grouped FC）：
	  - 核心思想，将输入和输出分为n组，每组单独处理对应的子空间
		
	-	超参数的可以由我们灵活指定，以在实际应用中平衡参数复杂度和模型性能

## softmax回归：
- 我们将模型的输出y视为是属于对应类的概率，然后选择具有最大输出值的类别作为我们的预测。然而我们并不能直接将线性预测的结果o作为输出，因为这样将会存在一些问题：
    - 【1】我们没有限制这些数字的总和为
    - 【2】这些值可以为负值，这违反了概率基本公式
为了解决这些问题我们引入了softmax函数。
- 核心思想：
  - 【1】将线性预测转换为概率分布
    - 通过线性模型计算每个类别的得分（称为logits），然后使用Softmax函数将这些得分转换为概率分布，确保所有类别的概率之和为1
  - 【2】Softmax函数<br>
![image](https://github.com/user-attachments/assets/a64accf7-a741-41a5-9527-9a02912c5e64)<br>
尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。	因此，softmax回归是一个**线性模型**（linear model）。


- 此外我们还需要一个训练的目标函数，来激励模型精准地估计概率，并且建立一个叫做校准（Calibration）的指标评价概率是否符合实际频率
-	通俗来说
  - 函数就像老师要求学生"考试时不确定就老实说概率，别瞎蒙"，我们需要一个评分规则（比如交叉熵损失函数）。<br>如果学生说"下雨概率90%"，结果没下雨，就会被扣很多分；如果说"下雨概率50%"，结果没下雨，扣分就少一点。这样训练久了，学生（模型）就会学会准确估计概率。
	- 校准就像在训练后的最终考试中统计学生的预测是否靠谱
 
## 损失函数：
- 使用与线性回归相同的最大似然估计（MLE），即寻找使观测数据最大化的参数
-对数似然：
  - softmax函数的输出结果是一个“对任意输入x的每个类的条件概率”的向量y ̂
  - 假设整个数据集{X,Y}有n个样本，我们规定需要最大化的似然函数为：
  - 每个样本对应的真实类的条件概率的乘积<br>
    ![image](https://github.com/user-attachments/assets/af5752ba-41d6-4117-9f94-22555c5c1896)
  - 最大化似然函数相当于最小化负对数似然函数：
    ![image](https://github.com/user-attachments/assets/d80ec042-e4d4-41c2-84ab-7d7bd7878291)
      - 注意log实际上是ln
      - 注意损失函数中的y是一个独热编码向量（即只有一个元素为1，其他为0），因此损失函数只考虑与实际标签对应的预测概率。
      - 这个损失函数通常被称为交叉熵损失，它所有标签分布的预期损失值

## 损失函数的导数
将损失函数与softmax函数联立得：<br>
![image](https://github.com/user-attachments/assets/52188974-acc5-407a-ad39-a3b8faa8821c)<br>
考虑对于未规范化的预测 $o_j$ 的导数有：
![image](https://github.com/user-attachments/assets/bd997209-9b13-411d-bcb0-8dcb41471c15)
- 这样看来导数是softmax模型分配的概率与实际发生的情况之间的差异。这与我们在回归中看到的是非常相似的。
- 这不是巧合，在任何指数分布模型中，对数似然的梯度正是由此得出的，这使得梯度计算在实践中变得容易很多。（进一步参见书中数学分布一节，中文版未翻译）

## 信息论
信息论涉及编码、解码、发生以及尽可能简洁地处理信息或数据
- 【1】熵
  - 信息论的核心思想是量化数据中的信息内容。<br>
		高熵：概率分布越均匀（如公平骰子），结果越难预测，信息量越大<br>
		低熵：概率分布越集中（如作弊骰子总出6），结果越确定，信息量越少
  - 在信息论中，该数值被称为分布P的熵。可以通过如下方程得到：<br>
    ![image](https://github.com/user-attachments/assets/bb651922-b766-4360-9efe-6f71bd7a55b3)
  - 方程的组成：<br>
    $P(j)$
    事件j发生的概率<br>
		$−log⁡P(j)$
    事件j的信息量，表示其意外程度（概率越低，信息量越大）<br>
		加权求和：对每个事件的自信息按概率加权平均，得到整体的平均信息量
  -	基于信息论的基本定理，为了对从分布p中随机抽取的数据进行编码，我们至少需要
    $H[P]$
    “纳特（nat）“对其进行编码。
  - 纳特相当于比特（bit），但是对数底为e而不是2。因此一个纳特约为1.44比特
- 【2】信息量
  - 上面说到
    $−log⁡P(j) $
    事件j的信息量，这是用来量化每一个时间发生时的惊异程度的。
	- 在观察一个事件j，并赋予它主观概率
    %P(j)$
    时，当一个事件的概率较低，该事件的信息量也就更大，在上面方程中定义的熵。
    $H[P]$
    ，表示的是当分配的概率真正匹配数据生成过程时的信息量的期望。
  - 理解分配的概率真正匹配数据生成过程时：
		  = [1]分配的概率（模型分布P(j)）：指我们主管设定的或模型预测的某个事件j的概率
		  - [2]数据生成过程的分布（真实分布Q(j)）：指数据实际服从的客观概率分布
		  - [3]匹配：当模型分布P(j)与真实分布Q(j)完全一致（即对所有的j，P(j)=Q(j)）
  - 【3】重新审视交叉熵
	  - 如果把熵H(P)想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？
		- 交叉熵从P到Q，记为<br>
    ![image](https://github.com/user-attachments/assets/1c3d4318-365e-4a9a-b2d2-e75dabcdc875)
      我们可以理解为“主观概率为Q（模型分布）的观察者在看到根据概率P(真实分布）生成的数据时的预期惊异程度“<br>
    - 当P≠Q时，交叉熵会比真实熵多出一个KL散度：
      ![image](https://github.com/user-attachments/assets/f4db5404-40d7-41e8-ac27-c4136c150e35)
   - P=Q时，交叉熵等于真实分布的熵H(P),即H(P,P)=H(P)，此时交叉熵达到最低<br>
     证明：<br>
     由于上文的损失函数与交叉熵函数相同，故由于<br>
     ![image](https://github.com/user-attachments/assets/947095e3-3fdb-412c-ad92-22557a334ac2)<br>
     原函数是Q的凸函数，根据凸优化理论，其最小值出现在梯度为0的点，即P=Q
## 模型预测和评估：
- 在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。
- 通常我们使用预测概率最高的类别作为**输出类别**。
- 如果预**测与实际类别（标签）一致，则预测是正确的**。
- 在接下来的实验中，我们将使用*精度*（accuracy）来评估模型的性能。
- 精度等于**正确预测数与预测总数之间的比率**。































